{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install fuzzywuzzy\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQJnzLSJCmgX"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch and related packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Import packages for data manipulation and data splitting\n",
        "import re\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Logging / metrics\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "# Downloading datasets, and loading\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Other\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhfNoW68ChxF",
        "outputId": "d99e4820-c705-4569-ecce-af179601882f"
      },
      "outputs": [],
      "source": [
        "# Fetch the 'adult' dataset from OpenML\n",
        "dataset = fetch_openml(name='SpeedDating', version=1)\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data=np.c_[dataset['data'], dataset['target']],\n",
        "                  columns=dataset['feature_names'] + ['target'])\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix grammatical errors in column names\n",
        "columns_to_rename = {\n",
        "    'd_sinsere_o': 'd_sincere_o',\n",
        "    'sinsere_o': 'sincere_o',\n",
        "    'intellicence_important': 'intelligence_important',\n",
        "    'd_intellicence_important': 'd_intelligence_important',\n",
        "    'ambtition_important': 'ambition_important',\n",
        "    'ambitous_o': 'ambition_o',\n",
        "    'd_ambitous_o': 'd_ambition_o',\n",
        "    'd_ambitous_important': 'd_ambition_important',\n",
        "    'pref_o_ambitious': 'pref_o_ambition'}\n",
        "\n",
        "df.rename(columns=columns_to_rename, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7C43JDAExqx",
        "outputId": "8e61dda4-2abe-4c53-c681-e0d469dba5ea"
      },
      "outputs": [],
      "source": [
        "def print_unique_for_column(df, column):\n",
        "    unique_values = df[column].unique()\n",
        "    sum = df[column].nunique(dropna=False)\n",
        "    print(f\"Unique values in column '{column}' with sum of {sum} (including NaN): {unique_values}\")\n",
        "\n",
        "def print_unique(df=df):\n",
        "    # Loop through each column and print unique values\n",
        "    for column in df.columns:\n",
        "        print_unique_for_column(df, column)\n",
        "\n",
        "print_unique(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column field has a lot of problems\n",
        "print(f\"Sum: {df['field'].nunique(dropna=True)}\")\n",
        "sorted_unique_values = sorted(df['field'].dropna().unique())\n",
        "for value in sorted_unique_values:\n",
        "    print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3vxhtV7F9Dz"
      },
      "outputs": [],
      "source": [
        "# Regular expression to match ;, :, -, /, and everything within []\n",
        "pattern = r'[;:\\-\\/]|\\[.*?\\]'\n",
        "\n",
        "# Replace matched patterns with an empty string\n",
        "df['field'] = df['field'].str.replace(pattern, ' ', regex=True)\n",
        "\n",
        "# Remove phd and remove duplicate space\n",
        "df['field'] = df['field'].str.replace('phd', '', flags=re.IGNORECASE, regex=True).str.replace(' +', ' ', regex=True)\n",
        "\n",
        "# Replace shortened engg. with engineering\n",
        "df['field'] = df['field'].str.replace('engg.', 'engineering', flags=re.IGNORECASE, regex=True)\n",
        "\n",
        "# Change everything to lowercase and remove white space\n",
        "df['field'] = df['field'].str.lower().str.strip()\n",
        "df['race'] = df['race'].str.lower().str.strip()\n",
        "df['race_o'] = df['race_o'].str.lower().str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to replace close matches\n",
        "def combine_similar(df, column, correct_value, threshold=90):\n",
        "    unique_values = df[column].unique()\n",
        "    \n",
        "    # Find matches above the threshold\n",
        "    matches = process.extract(correct_value, unique_values, limit=None, scorer=fuzz.token_sort_ratio)\n",
        "    close_matches = [match[0] for match in matches if match[1] >= threshold]\n",
        "    \n",
        "    # Replace close matches with the correct value\n",
        "    df[column] = df[column].apply(lambda x: correct_value if x in close_matches else x)\n",
        "\n",
        "# Mostly used to fix grammatical errors in some strings\n",
        "combine_similar(df, 'field', 'finance')\n",
        "combine_similar(df, 'field', 'nutrition')\n",
        "combine_similar(df, 'field', 'speech language pathology')\n",
        "combine_similar(df, 'field', 'international affairs')\n",
        "combine_similar(df, 'field', 'finance economics')\n",
        "combine_similar(df, 'field', 'mathematic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW58FGbXM1Wo",
        "outputId": "98b7430a-8cdd-4e7c-cc26-f6c018c5edc5"
      },
      "outputs": [],
      "source": [
        "# Dropped from 259 values to 203 not including NaN\n",
        "print(f\"Sum: {df['field'].nunique(dropna=True)}\")\n",
        "sorted_unique_values = sorted(df['field'].dropna().unique())\n",
        "for value in sorted_unique_values:\n",
        "    print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def try_convert_float(value):\n",
        "    try:\n",
        "        return float(value)\n",
        "    except ValueError:\n",
        "        return value\n",
        "\n",
        "# Some columns have numbers in string type so we use his to convert them to float\n",
        "for column in df.columns:\n",
        "  df[column] = df[column].apply(try_convert_float)\n",
        "\n",
        "print_unique(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filling the NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check how much of percentage is missing from every column\n",
        "missing_values = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "for name, value in missing_values.items():\n",
        "  print(f\"{name}: {value:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column 'expected_num_interested_in_me' has a lot of missing values so we drop it\n",
        "# We experimented by leaving it in but id did not change anything\n",
        "columns_to_drop = ['has_null', 'wave', 'expected_num_interested_in_me']\n",
        "\n",
        "df_features = df.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will select columns with data type 'object' or 'string'\n",
        "string_columns = df_features.select_dtypes(include=['object', 'string'])\n",
        "\n",
        "print_unique(string_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nominal_columns = ['gender', 'race', 'race_o', 'field']\n",
        "ordinal_columns = string_columns.drop(columns=nominal_columns)\n",
        "\n",
        "# Encode with one hot encoder\n",
        "df_encoded = pd.get_dummies(df_features, columns=nominal_columns, drop_first=False)\n",
        "\n",
        "# Define the order of categories\n",
        "order = {'[0-1]': 0.0, '[2-3]': 1.0, '[4-6]': 2.0, '[7-37]': 3.0,\n",
        "         '[0-1]': 0.0, '[2-5]': 1.0, '[6-10]': 2.0,\n",
        "         '[0-2]': 0.0, '[3-5]': 1.0, '[5-18]': 2.0,\n",
        "         '[0-3]': 0.0, '[4-9]': 1.0, '[10-20]': 2.0,\n",
        "         '[0-4]': 0.0, '[5-6]': 1.0, '[7-10]': 2.0,\n",
        "         '[0-5]': 0.0, '[6-8]': 1.0, '[9-10]': 2.0, \n",
        "         '[0-15]': 0.0, '[16-20]': 1.0, '[21-100]': 2.0,\n",
        "         '[-1-0]': 0.0, '[0-0.33]': 1.0, '[0.33-1]' : 2.0}\n",
        "\n",
        "# Encode with ordinal encoder\n",
        "for column in ordinal_columns:\n",
        "    df_encoded[column] = df_encoded[column].map(order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if there are any more left\n",
        "print_unique(df_encoded.select_dtypes(include=['object', 'string']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting box plots to check outliers\n",
        "df_encoded.plot(kind='box', figsize=(40, 20), vert=False)\n",
        "plt.title('Box plot of all columns')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization of data\n",
        "normalizer = MinMaxScaler()\n",
        "\n",
        "df_normalized = normalizer.fit_transform(df_encoded)\n",
        "df_normalized = pd.DataFrame(df_normalized, columns=df_encoded.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill the missing data with KNN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "imputed_data = imputer.fit_transform(df_normalized)\n",
        "clean_data = pd.DataFrame(imputed_data, columns=df_encoded.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the absolute difference between preferences and ratings\n",
        "compatibility_columns = ['attractive', 'sincere', 'intelligence', 'funny', 'ambition', 'shared_interests']\n",
        "\n",
        "for column in compatibility_columns:\n",
        "  clean_data[f\"{column}_compatibility_score\"] = (\n",
        "      (1 - abs(clean_data[f\"pref_o_{column}\"] - clean_data[f\"{column}_partner\"])) +\n",
        "      (1 - abs(clean_data[f\"{column}_important\"] - clean_data[f\"{column}_o\"]))\n",
        "  ) / 2\n",
        "\n",
        "  print_unique_for_column(clean_data, f\"{column}_compatibility_score\")\n",
        "\n",
        "clean_data['overall_compatibility'] = clean_data[[col + '_compatibility_score' for col in compatibility_columns]].mean(axis=1)\n",
        "print_unique_for_column(clean_data, 'overall_compatibility')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = clean_data.drop('target', axis=1)\n",
        "y = clean_data['target']\n",
        "\n",
        "mic = mutual_info_classif(X, y)\n",
        "\n",
        "mic_series = pd.Series(mic, index=X.columns)\n",
        "mic_series = mic_series.sort_values(ascending=False)\n",
        "\n",
        "# Get only columns that are above the value\n",
        "mic_series = mic_series[mic_series > 0.01]\n",
        "\n",
        "# Show best correlations\n",
        "mic_series.plot.bar(figsize=(15, 4))\n",
        "plt.ylabel('Mutual Information Score')\n",
        "plt.xlabel('Features')\n",
        "plt.title('Mutual Information Scores')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use only selected\n",
        "# features_data = clean_data[mic_series.index.tolist()]\n",
        "\n",
        "# Got better results with all of the features\n",
        "features_data = clean_data.drop('target', axis=1)\n",
        "target_data = clean_data['target']\n",
        "print(f\"Features data shape is: {features_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the feature set X and the target variable y\n",
        "X = features_data\n",
        "y = target_data\n",
        "\n",
        "# Let's split the data into training (80%) and test (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50, stratify=y)\n",
        "\n",
        "# Verify the shape of each set\n",
        "print(f\"Train set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert Pandas DataFrames to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDataset for each set\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoader for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "# Verify the DataLoader objects\n",
        "for inputs, labels in train_loader:\n",
        "    print(f\"Batch shape: {inputs.shape}, Label shape: {labels.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If available use GPU instead of CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size: int = len(X.columns), hidden_size: int = 256, dropout_ratio: float = 0.2):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Hidden layer\n",
        "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.hidden_layer1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout_layer1 = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        # Additional hidden layer\n",
        "        self.hidden_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout_layer2 = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.hidden_layer1(x) + x\n",
        "        x = self.norm1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.dropout_layer1(x)\n",
        "        \n",
        "        x = self.hidden_layer2(x) + x\n",
        "        x = self.norm2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.dropout_layer2(x)\n",
        "        \n",
        "        x = self.output_layer(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = SimpleMLP().to(device)\n",
        "optim = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the number of matched and not-matched samples\n",
        "total_positive_samples = target_data[target_data == 1].count()\n",
        "total_negative_samples = target_data[target_data == 0].count()\n",
        "\n",
        "pos_weight = total_negative_samples / total_positive_samples\n",
        "pos_weight_tensor = torch.tensor(pos_weight).float().to(device)\n",
        "\n",
        "# Give more importance to the minority class\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with wandb.init(\n",
        "    project=\"Speed Dating\",\n",
        "    config={\n",
        "    \"Architecture\": \"MLP\",\n",
        "    \"Epochs\": 50,\n",
        "    \"Learning rate\": 0.001,\n",
        "    \"Betas\": [0.9, 0.999],\n",
        "    \"Dataset\": \"SpeedDating\",\n",
        "    \"Optimizer\": \"Adam\",\n",
        "    \"Training batch\": 64,\n",
        "    \"Test batch\": 512,\n",
        "    \"Hidden size\": 256,\n",
        "    \"Dropout ratio\": 0.2\n",
        "    },\n",
        ") as run:\n",
        "    train_losses = deque(maxlen=50)\n",
        "    epochs = 50\n",
        "    saved_weights = []\n",
        "    saved_better_weights = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        epoch_train_loss = []\n",
        "\n",
        "        model.train()\n",
        "        with tqdm(total=len(train_loader), position=0, leave=True) as pbar:\n",
        "            for x, y in train_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                optim.zero_grad()\n",
        "                out = model(x)\n",
        "                loss = loss_fn(out, y.unsqueeze(-1))\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "                epoch_train_loss.append(loss.item())\n",
        "\n",
        "                pbar.set_postfix_str(f\"Epoch: {i}, Avg. train loss: {sum(train_losses) / len(train_losses)}\")\n",
        "                pbar.update(1)\n",
        "\n",
        "        print(f\"Epoch Train Loss: {sum(epoch_train_loss) / len(epoch_train_loss)}\")\n",
        "        run.log({\"Epoch Train Loss\": sum(epoch_train_loss) / len(epoch_train_loss)}, step=i)\n",
        "\n",
        "        # Save weights\n",
        "        saved_weights.append(model.state_dict().copy())\n",
        "\n",
        "        all_probs = []  # Store probabilities for ROC AUC\n",
        "        all_preds = []  # Store binary predictions for F1\n",
        "        all_labels = []\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        model.eval()\n",
        "        with torch.inference_mode():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                probs = torch.sigmoid(out)\n",
        "                preds = (probs > 0.5).float()\n",
        "                \n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "                correct += (preds == y.unsqueeze(1)).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {accuracy}\")\n",
        "        run.log({'Accuracy': accuracy}, step=i)\n",
        "        \n",
        "        roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "        print(f\"Test ROC AUC Score: {roc_auc}\")\n",
        "        run.log({'ROC AUC Score': roc_auc}, step=i)\n",
        "\n",
        "        f1 = f1_score(all_labels, np.round(all_preds))\n",
        "        print(f\"Test F1 Score: {f1}\")\n",
        "        run.log({'F1 Score': f1}, step=i)\n",
        "\n",
        "        # Save weights above the value\n",
        "        if (f1 > 0.52):\n",
        "            saved_better_weights.append(model.state_dict().copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_average_weights(weights):\n",
        "    average_weights = {}\n",
        "\n",
        "    for key in weights[0].keys():\n",
        "        average_weights[key] = torch.zeros_like(weights[0][key])\n",
        "\n",
        "    for state_dict in weights:\n",
        "        for key in state_dict.keys():\n",
        "            average_weights[key] += state_dict[key]\n",
        "\n",
        "    for key in average_weights.keys():\n",
        "        average_weights[key] = average_weights[key].float() / len(weights)\n",
        "\n",
        "    return average_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a new model using the averaged weights\n",
        "# This method loads the state dictionary into the model.\n",
        "# The state dictionary contains the weights of the model.\n",
        "average_weights_model = SimpleMLP().to(device)\n",
        "average_weights = calculate_average_weights(saved_weights)\n",
        "average_weights_model.load_state_dict(average_weights)\n",
        "\n",
        "all_probs = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out = average_weights_model(x)\n",
        "        probs = torch.sigmoid(out)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        correct += (preds == y.unsqueeze(1)).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "print(f\"Test ROC AUC Score: {roc_auc}\")\n",
        "\n",
        "f1 = f1_score(all_labels, np.round(all_preds))\n",
        "print(f\"Test F1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a new model using the averaged weights that were above a value\n",
        "average_weights_model = SimpleMLP().to(device)\n",
        "average_weights_model.load_state_dict(calculate_average_weights(saved_better_weights))\n",
        "\n",
        "all_probs = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out = average_weights_model(x)\n",
        "        probs = torch.sigmoid(out)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        correct += (preds == y.unsqueeze(1)).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "print(f\"Test ROC AUC Score: {roc_auc}\")\n",
        "\n",
        "f1 = f1_score(all_labels, np.round(all_preds))\n",
        "print(f\"Test F1 Score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
